# Home Depot Reranker Fineâ€‘Tuning Plan

## Introduction

This document describes how to fineâ€‘tune the **Qwen/Qwen3â€‘Rerankerâ€‘0.6B** model on the Home Depot product search dataset.  The goal is to learn a model that accepts a user query and a candidate product description and outputs a relevance score.  The Qwen reranker is a **generative, crossâ€‘encoder reranking model** built on the Qwen3 foundation.  Crossâ€‘encoders process both the query and the document simultaneously using selfâ€‘attention and are therefore able to capture fineâ€‘grained interactions between tokensã€493181220393779â€ L95-L103ã€‘.  Qwen rerankers are implemented as **causal language models**; they judge relevance by comparing the logits of special tokens (â€œyesâ€ vs. â€œnoâ€) at the final positionã€967128664425397â€ L118-L125ã€‘.  Training these models requires converting each queryâ€“document pair into a prompt that ends with a fixed instruction; the model should learn to generate **â€œyesâ€** when the product is relevant and **â€œnoâ€** otherwise.

The instructions below are suitable for executing in **CURSOR IDE**.  They cover dataset preparation, building a custom training script, evaluating the model before and after fineâ€‘tuning, and provide guidance on hyperâ€‘parameter choices and best practices.  Example code is written in Python using **PyTorch**, **Transformers (â‰¥Â 4.51)** and **Datasets**.  Advanced users can optionally leverage the **ModelScope SWIFT** library for listwise training, but this plan focuses on a pointwise binaryâ€‘classification approach.

## 1. Environment Setup

1. **Python Environment** â€“ Create a new virtual environment and install dependencies.  Use PythonÂ 3.10 or later.

   ```sh
   python3 -m venv venv
   source venv/bin/activate
   pip install --upgrade pip
   # core libraries
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   pip install transformers==4.51.0 datasets evaluate scikitâ€‘learn
   # optional for logging & monitoring
   pip install tensorboard wandb
   ```

2. **GPU and Mixed Precision** â€“ The Qwen3â€‘Rerankerâ€‘0.6B has ~600Â million parameters and supports 32Â k token contexts.  To train efficiently you should have at least one 24Â GB or larger GPU.  Enable **flashâ€‘attention** in Transformers by passing `attn_implementation="flash_attention_2"` when loading the modelã€90629648899456â€ L172-L180ã€‘.  Mixed precision (FP16 or BF16) can reduce memory usage and should be turned on if supported.

3. **Dataset Files** â€“ Ensure that the Home Depot dataset (e.g., `home_depot.json`) and this training plan are available in your project.  The dataset contains 74Â 067 queryâ€“product pairs with fields: `id`, `entity_id`, `name`, `query`, `relevance`, and `description`.  Each product appears in multiple query contexts and relevance scores range from 1.0Â toÂ 3.0 (13 distinct levels).

4. **Directory Structure** â€“ Organise your project as follows:

   ```text
   project_root/
   â”œâ”€â”€ data/
   â”‚   â””â”€â”€ home_depot.json            # raw dataset
   â”œâ”€â”€ scripts/
   â”‚   â”œâ”€â”€ train_reranker.py         # training script
   â”‚   â””â”€â”€ evaluate_reranker.py      # evaluation script
   â”œâ”€â”€ models/
   â”‚   â””â”€â”€ checkpoints/              # fineâ€‘tuned models saved here
   â””â”€â”€ logs/                         # tensorboard logs
   ```

## 2. Dataset Preparation

### 2.1 Loading and Splitting

1. **Load the JSON dataset** using the ğŸ¤—Â Datasets library:

   ```python
   from datasets import load_dataset, Dataset

   data = load_dataset('json', data_files='data/home_depot.json', split='train')
   # Inspect fields
   print(data.features)
   ```

2. **Group by query for splitting.**  To avoid leaking information across splits, keep all products belonging to the same query in the same partition.  Stratify by average relevance to preserve score distribution.  A common split is 70Â % train, 15Â % validation, 15Â % test.  Here is an example splitting function:

   ```python
   import numpy as np
   import pandas as pd
   from sklearn.model_selection import train_test_split

   # Convert to pandas for grouping
   df = data.to_pandas()
   # Compute average relevance per query
   query_groups = df.groupby('query')['relevance'].mean().reset_index(name='avg_rel')
   # Stratify by binned average relevance
   bins = [0, 1.67, 2.0, 2.33, 3.0]
   query_groups['bin'] = pd.cut(query_groups['avg_rel'], bins=bins, labels=False)
   train_queries, temp_queries = train_test_split(query_groups, test_size=0.3, stratify=query_groups['bin'], random_state=42)
   val_queries, test_queries = train_test_split(temp_queries, test_size=0.5, stratify=temp_queries['bin'], random_state=42)

   # Filter original dataframe
   train_df = df[df['query'].isin(train_queries['query'])]
   val_df   = df[df['query'].isin(val_queries['query'])]
   test_df  = df[df['query'].isin(test_queries['query'])]

   # Convert back to datasets
   train_data = Dataset.from_pandas(train_df)
   val_data   = Dataset.from_pandas(val_df)
   test_data  = Dataset.from_pandas(test_df)
   ```

3. **Sanity check** â€“ Verify that no query is shared across splits and that relevance score distributions in each set mirror the global distribution (see dataset report).  Use histograms or counts by bin.

### 2.2 Preâ€‘processing Text

The Qwen generative reranker expects a **messages**â€‘based input.  Each sample must be converted into a system/user prompt template ending with a `yes`/`no` answer.  The default template described in ModelScopeâ€™s documentation isã€967128664425397â€ L250-L260ã€‘:

```text
<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: {Instruction}
<Query>: {Query}
<Document>: {Document}<|im_end|>
<|im_start|>assistant
<think>

</think>

```

For this fineâ€‘tuning task we keep the default instruction:

```text
Given a web search query, retrieve relevant passages that answer the query
```

1. **Construct document text** by concatenating the product name and a truncated description.  Descriptions vary from 153Â toÂ 5Â 516 characters (median ~885) and often exceed the modelâ€™s context limit.  Extract the first 256â€“384 tokens of the description while preserving entire bullet points.  Example:

   ```python
   import re
   from transformers import AutoTokenizer

   tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Reranker-0.6B')

   def truncate_description(text, max_tokens=350):
       tokens = tokenizer.encode(text, add_special_tokens=False)
       if len(tokens) > max_tokens:
           tokens = tokens[:max_tokens]
       return tokenizer.decode(tokens, skip_special_tokens=True)

   def build_document(row):
       desc = truncate_description(row['description'])
       return f"{row['name']}. {desc}"

   for split_name, split in [('train', train_data), ('validation', val_data), ('test', test_data)]:
       split = split.add_column('document', [build_document(row) for row in split])
   ```

2. **Lowercase normalization** â€“ Convert queries and documents to lowercase (97Â % of queries are already lowercase) to reduce vocabulary size.  Do **not** remove numbers or special characters because they convey important product specifications.

3. **Label engineering** â€“ Convert the 13â€‘level relevance scores into binary labels suitable for a generative classification reranker.  A simple mapping is:
   * **relevant (1)** if `relevance â‰¥ 2.33` (good, very good, perfect match) â€“ roughly top 46Â % of samples.
   * **irrelevant (0)** if `relevance < 2.33`.

   This threshold balances positive and negative classes while still leaving enough negative examples.  Save the binary label in a new column `label`.  For more nuanced models you can map scores to continuous probabilities: `p = (relevanceÂ âˆ’Â 1.0) / 2.0`, then use meanâ€‘squared error instead of crossâ€‘entropy.

4. **Generate message dictionaries** â€“ For each sample create a dictionary with the query and document formatted for the generative reranker:

   ```python
   def build_message(example, instruction="Given a web search query, retrieve relevant passages that answer the query"):
       user_content = f"<Instruct>: {instruction}\n<Query>: {example['query']}\n<Document>: {example['document']}"
       # positive_messages contains a single answer; the model is expected to output "yes" for relevant and "no" otherwise
       return {
           'messages': [{'role': 'user', 'content': user_content}],
           'positive_messages': [[{'role': 'assistant', 'content': 'yes'}]] if example['label'] == 1 else [],
           'negative_messages': [[{'role': 'assistant', 'content': 'no'}]] if example['label'] == 0 else []
       }

   train_msgs = train_data.map(build_message)
   val_msgs   = val_data.map(build_message)
   test_msgs  = test_data.map(build_message)
   ```

   The format above mirrors the `messages`, `positive_messages` and `negative_messages` fields used by ModelScope SWIFTã€967128664425397â€ L182-L228ã€‘.  Only one positive or negative message is provided per sample; SWIFT will group multiple negatives automatically.  The binary label is implied by the presence of the positive or negative message.

### 2.3 Negative Sampling (optional)

To strengthen the model, include **hard negatives**, i.e., nonâ€‘relevant products that are similar to the query.  SentenceÂ Transformers provides a `mine_hard_negatives` utility for this purposeã€938936884683624â€ L281-L334ã€‘.  You can use a lightweight embedding model (e.g., `sentence-transformers/static-retrieval-mrl-en-v1`) to find hard negatives within each query group and add them to `negative_messages`.  This step improves the modelâ€™s ability to discriminate between very similar products but is optional if computational resources are limited.

## 3. Model Loading

The Qwen reranker is a generative language model.  We load it via `AutoModelForCausalLM` and `AutoTokenizer`.  The model expects the query and document to be prefaced by a **system message** and uses `yes` and `no` tokens to compute relevance probabilityã€90629648899456â€ L172-L190ã€‘.  The following code shows how to load the model and set up special tokens:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = 'Qwen/Qwen3-Reranker-0.6B'
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    attn_implementation='flash_attention_2'
).cuda()

token_true_id  = tokenizer.convert_tokens_to_ids('yes')
token_false_id = tokenizer.convert_tokens_to_ids('no')
max_length = 8192  # maximum sequence length supported by the model

# Prepare prefix and suffix tokens for the template
prefix = '<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
suffix = '<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n'
prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)
suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)
```

The Qwen reranker uses left padding; ensure that `tokenizer.padding_side` is set accordingly.  Enabling flash attention accelerates attention computations, as suggested in the model cardã€90629648899456â€ L172-L180ã€‘.

## 4. Building the Training Script (train_reranker.py)

### 4.1 Data Collator

Define a collator that converts the message dictionaries into token IDs with the proper prefix and suffix.  Each input sequence should respect the modelâ€™s context length (8192 tokens).  Unused tokens should be padded on the left so that the final tokens correspond to the answer.  For each sample, we return input IDs and the **target label** (1Â for relevant â†’ â€œyesâ€, 0Â for irrelevant â†’ â€œnoâ€).

```python
from torch.utils.data import Dataset, DataLoader
from transformers import DataCollatorForSeq2Seq

class RerankerDataset(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        return self.dataset[idx]

def collate_fn(batch):
    messages = []
    labels   = []
    for example in batch:
        # Build instruction prompt
        user_content = example['messages'][0]['content']
        full_text   = prefix + user_content + suffix
        # Tokenize and truncate from the left if necessary
        input_ids = tokenizer.encode(full_text, add_special_tokens=False, truncation=True, max_length=max_length - 1)
        # Append the answer token placeholder; we will teach the model to generate 'yes' or 'no'
        input_ids = prefix_tokens + input_ids + suffix_tokens
        input_ids = input_ids[-max_length:]
        # Pad on the left
        padding_length = max_length - len(input_ids)
        input_ids = [tokenizer.pad_token_id] * padding_length + input_ids
        messages.append(torch.tensor(input_ids, dtype=torch.long))
        # Determine label: 1 if positive_messages present, else 0
        label = 1 if example['positive_messages'] else 0
        labels.append(label)
    batch_input_ids = torch.stack(messages)
    batch_labels    = torch.tensor(labels, dtype=torch.float32)
    return {'input_ids': batch_input_ids.cuda(), 'labels': batch_labels.cuda()}

train_dataset = RerankerDataset(train_msgs)
val_dataset   = RerankerDataset(val_msgs)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, drop_last=True)
val_loader   = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)
```

Notes:

* The batch size of 2 is chosen for demonstration.  In practice you should use the largest batch size that fits into GPU memory.  Because Qwen uses 32k tokens, each sequence consumes significant memory.  If memory becomes a bottleneck, use **gradient accumulation** to achieve a larger effective batch size.

* Each input ends with the `<think>` tag; the model will output the next token (either â€œyesâ€ or â€œnoâ€) at the last position.  We ignore the â€œpositive_messagesâ€ or â€œnegative_messagesâ€ text because the generative reranker uses only the tokens â€œyesâ€ and â€œnoâ€ at inference timeã€967128664425397â€ L118-L125ã€‘.

### 4.2 Loss Function

For pointwise training we treat the task as **binary classification**: for each queryâ€“document pair the model should generate â€œyesâ€ if the product is relevant and â€œnoâ€ otherwise.  This matches the pointwise loss definition in the SWIFT documentationã€967128664425397â€ L131-L141ã€‘.  We compute the binary crossâ€‘entropy between the modelâ€™s predicted probability for the positive token and the target label.

```python
from torch.nn import functional as F

def compute_loss(logits, labels):
    # logits: [batch_size, vocab_size] at the last position
    # Extract logits for "yes" and "no"
    true_logits  = logits[:, token_true_id]
    false_logits = logits[:, token_false_id]
    # Compute probability that the model chooses "yes"
    probs = torch.sigmoid(true_logits - false_logits)
    # Binary crossâ€‘entropy loss
    loss = F.binary_cross_entropy(probs, labels)
    return loss
```

### 4.3 Training Loop

The following skeleton illustrates a simple training loop with gradient accumulation, learning rate scheduling, and mixed precision.  Save this script as `scripts/train_reranker.py`.

```python
import torch
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

num_epochs = 3
accum_steps = 8  # accumulate gradients to simulate larger batch
learning_rate = 5e-6
warmup_ratio  = 0.1

optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
total_steps = len(train_loader) // accum_steps * num_epochs
warmup_steps = int(total_steps * warmup_ratio)
scheduler   = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)

model.train()
for epoch in range(num_epochs):
    total_loss = 0.0
    for step, batch in enumerate(train_loader):
        input_ids = batch['input_ids']
        labels    = batch['labels']
        # Forward pass with labels masked so the model only predicts at final position
        outputs = model(input_ids)
        logits = outputs.logits[:, -1, :]
        loss   = compute_loss(logits, labels) / accum_steps
        loss.backward()
        total_loss += loss.item()
        if (step + 1) % accum_steps == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
        if (step + 1) % 100 == 0:
            print(f"Epoch {epoch+1} step {step+1}: loss={total_loss/(step+1):.4f}")
    # Validation after each epoch
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            outputs = model(batch['input_ids'])
            logits  = outputs.logits[:, -1, :]
            loss    = compute_loss(logits, batch['labels'])
            val_loss += loss.item()
    avg_val_loss = val_loss / len(val_loader)
    print(f"Epoch {epoch+1} validation loss: {avg_val_loss:.4f}")
    model.train()
    # Save checkpoint
    model.save_pretrained(f"models/checkpoints/epoch_{epoch+1}")
```

**Hyperâ€‘parameters** â€“ You may adjust the learning rate (5eâ€‘6 to 2eâ€‘5) and number of epochs (3â€“5) based on validation loss.  Use early stopping if the validation loss stops improving.  Since the dataset is balanced across high and low relevance scores, you usually do not need label weighting, but you can weight the positive class if needed.  Warmâ€‘up for 10Â % of the total steps helps stabilise training.

### 4.4 Advanced: Listwise Training with SWIFT

ModelScopeâ€™s **SWIFT** framework supports **listwise** generative reranking, where each query is associated with one positive document and multiple negatives; the model learns to choose the positive among themã€967128664425397â€ L149-L177ã€‘.  To use SWIFT for the Home Depot dataset:

1. Install SWIFT: `pip install ms-swift`.
2. Convert the dataset into the **LLM reranker format** described in the documentationã€967128664425397â€ L182-L228ã€‘.  For each query, identify one positive (highestâ€‘scoring) product and at most seven negatives (lowestâ€‘scoring) products.  Example entry:

   ```json
   {
     "messages": [{"role": "user", "content": "<Instruct>: Given a web search query, retrieve relevant passages that answer the query\n<Query>: angle bracket\n<Document>: Simpson Strongâ€‘Tie 12â€‘Gauge Angle. Not only do angles make joints stronger..."}],
     "positive_messages": [[{"role": "assistant", "content": "yes"}]],
     "negative_messages": [[{"role": "assistant", "content": "no"}], [{"role": "assistant", "content": "no"}], ...]
   }
   ```

3. Run SWIFTâ€™s training script.  For pointwise classification, use the `generative_reranker` loss; for listwise ranking, use `listwise_generative_reranker`.  Example command (adjust `model`, `output_dir`, and batch sizes):

   ```sh
   nproc_per_node=2
   swift sft \
       --model Qwen/Qwen3-Reranker-0.6B \
       --task_type generative_reranker \
       --loss_type generative_reranker \
       --train_type full \
       --dataset path/to/home_depot_reranker_dataset.json \
       --split_dataset_ratio 0.1 \
       --output_dir models/swift_checkpoint \
       --num_train_epochs 3 \
       --per_device_train_batch_size 1 \
       --gradient_accumulation_steps 16 \
       --learning_rate 6e-6 \
       --eval_strategy steps \
       --eval_steps 200 \
       --save_steps 1000 \
       --label_names labels \
       --dataloader_drop_last true
   ```

SWIFT handles prompt formatting internally and implements both pointwise and listwise losses.  Use `MAX_POSITIVE_SAMPLES` and `MAX_NEGATIVE_SAMPLES` environment variables to control the number of examples per queryã€967128664425397â€ L210-L229ã€‘.  Monitor GPU memory usage and adjust `gradient_accumulation_steps` accordingly.

## 5. Evaluation

### 5.1 Computing Relevance Scores

After fineâ€‘tuning, evaluate the modelâ€™s performance on the heldâ€‘out test set.  To compute relevance scores for each queryâ€“document pair, follow the inference example from the model cardã€90629648899456â€ L168-L213ã€‘:

```python
@torch.no_grad()
def compute_scores(model, input_ids_batch):
    outputs = model(input_ids_batch)
    logits  = outputs.logits[:, -1, :]
    true_logits  = logits[:, token_true_id]
    false_logits = logits[:, token_false_id]
    probs = torch.sigmoid(true_logits - false_logits)
    return probs.cpu().numpy()

test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)
model.eval()
all_scores = []
all_labels = []
for batch in test_loader:
    scores = compute_scores(model, batch['input_ids'])
    all_scores.extend(scores)
    all_labels.extend(batch['labels'].cpu().numpy())
```

Scores range between 0 and 1 and represent the modelâ€™s confidence that the product matches the query.  To obtain the final ranking for a query, group candidates by query and sort by descending score.  Optionally, rescale scores back to the original 1â€“3 relevance range using `scaled = 1 + 2 * score`.

### 5.2 Ranking Metrics

Compute ranking metrics such as **NDCG@10**, **MAP**, **MRR**, and **Precision@K** as recommended in the dataset research.  The `evaluate` library or `scikitâ€‘learn` can be used for this purpose.  For example:

```python
from collections import defaultdict
import numpy as np

def compute_metrics(scores, labels, queries):
    # group by query
    groups = defaultdict(list)
    for s, l, q in zip(scores, labels, queries):
        groups[q].append((s, l))
    ndcg_values = []
    map_values  = []
    mrr_values  = []
    for q, pairs in groups.items():
        pairs_sorted = sorted(pairs, key=lambda x: x[0], reverse=True)
        rels = [l for _, l in pairs_sorted]
        # DCG@10
        dcg = sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(rels[:10]))
        # Ideal DCG
        ideal = sorted(rels, reverse=True)
        idcg = sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(ideal[:10])) or 1
        ndcg_values.append(dcg / idcg)
        # MAP
        hits = 0
        precisions = []
        for i, rel in enumerate(rels):
            if rel > 0:
                hits += 1
                precisions.append(hits / (i + 1))
        map_values.append(np.mean(precisions) if precisions else 0)
        # MRR
        try:
            first_rel = rels.index(1)
            mrr_values.append(1 / (first_rel + 1))
        except ValueError:
            mrr_values.append(0)
    return {
        'NDCG@10': np.mean(ndcg_values),
        'MAP': np.mean(map_values),
        'MRR': np.mean(mrr_values)
    }

metrics = compute_metrics(all_scores, all_labels, list(test_df['query']))
print(metrics)
```

Compare metrics **before** fineâ€‘tuning (using the base Qwen3 reranker) and **after** fineâ€‘tuning.  A strong model should significantly improve NDCG@10, MAP and MRR over the baseline (e.g., aiming for NDCG@10 â‰¥Â 0.80 as suggested in the dataset report).

### 5.3 Error Analysis

1. **Relevance Level Analysis** â€“ Segment test cases by their human relevance scores (1.0â€“3.0) and examine whether the model struggles at the boundaries (e.g., distinguishing 2.0 vs. 2.33).  Compare predicted scores across these segments.
2. **Query Type Analysis** â€“ Use the query characteristics from the dataset report (brand vs. specification vs. typo queries) to evaluate performance on different query types.  This can inform targeted augmentation or weighting strategies.
3. **Failure Modes** â€“ Inspect high scoring false positives and low scoring false negatives to understand whether the model is misled by certain features (e.g., synonyms, synonyms in descriptions, or unusual brand names).  Use these insights to design further fineâ€‘tuning or data cleaning.

## 6. Postâ€‘Training Deployment

1. **Model Quantisation** â€“ For production inference consider converting the fineâ€‘tuned model to **INT8** or **4â€‘bit** weights using [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) or `transformers.quantization`.  Quantisation reduces memory usage and speeds up inference with minimal accuracy loss.
2. **Serving** â€“ Deploy the model behind a web service.  You can use **vLLM** for highâ€‘throughput generation; the model card provides vLLM usage exampleã€90629648899456â€ L215-L309ã€‘.  For simpler deployments, wrap inference in a FastAPI or Flask service with GPU support.
3. **Pipeline Integration** â€“ In a twoâ€‘stage retrieval system, first use a fast dense retriever (e.g., Qwen3â€‘Embedding) to fetch topâ€‘100 candidates, then apply the fineâ€‘tuned reranker to refine the topâ€‘k results.  This balances efficiency and precision.

## 7. Tips and Best Practices

1. **Instruction Tuning** â€“ Qwen rerankers support userâ€‘defined instructions.  The model card notes that using an instruction generally improves retrieval performance by 1Â %â€“5Â %ã€90629648899456â€ L314-L317ã€‘.  Experiment with domainâ€‘specific instructions (e.g., â€œGiven a shopperâ€™s query, determine whether the product matches the shopperâ€™s intentâ€) and include them in the prompt.
2. **Loss Variants** â€“ If binary classification is too coarse, map relevance scores to continuous probabilities and train with mean squared error or Huber loss.  You can also discretize scores into more than two bins and use multiâ€‘class crossâ€‘entropy with tokens â€œlowâ€, â€œmediumâ€, â€œhighâ€.
3. **Negative Sampling** â€“ Mining hard negatives using an embedding model helps the reranker learn subtle distinctionsã€938936884683624â€ L281-L334ã€‘.  Balance easy and hard negatives to avoid overfitting.
4. **Crossâ€‘Validation** â€“ Consider 5â€‘fold queryâ€‘stratified crossâ€‘validation to obtain robust performance estimates.  The average across folds gives a reliable picture of generalisation.
5. **Monitoring & Logging** â€“ Use TensorBoard or WeightsÂ &Â Biases to monitor training loss, validation loss, and evaluation metrics.  This helps catch overfitting and compare runs.
6. **LoRA/PEFT** â€“ To reduce fineâ€‘tuning costs, apply Lowâ€‘Rank Adaptation (LoRA) or QLoRA to only train a small number of adapter parameters while keeping the base model frozen.  The Qwen3 reranker accepts LoRA adapters (via `peft` library) because it is built on the same underlying architectureã€493181220393779â€ L95-L103ã€‘.
7. **Ethical Considerations** â€“ Ensure that the model does not inadvertently encode bias or present discriminatory results.  Evaluate fairness across product categories and check for spurious correlations.

## 8. Conclusion

This plan outlines a complete workflow for fineâ€‘tuning the **Qwen3â€‘Rerankerâ€‘0.6B** on the Home Depot product search dataset.  By preparing the data carefully, constructing appropriate promptâ€‘based messages, and using a pointwise binary classification loss, you can teach the model to recognise relevant products with high precision.  Optionally, SWIFT enables more advanced listwise training with relative ranking losses.  Robust evaluation and error analysis ensure that the fineâ€‘tuned model meets performance targets and yields insight for further improvement.

**References**

* Qwen3 Embedding blog â€“ highlights the dual/crossâ€‘encoder architecture of reranking modelsã€493181220393779â€ L95-L103ã€‘ and notes that reranker models are trained on highâ€‘quality labelled dataã€493181220393779â€ L117-L124ã€‘.
* Qwen3â€‘Reranker model card â€“ shows how to construct prompts, use â€œyesâ€/â€œnoâ€ tokens, and suggests enabling flash attention for better performanceã€90629648899456â€ L172-L190ã€‘.
* ModelScope SWIFT documentation â€“ explains that generative rerankers compute the probability of â€œyesâ€/â€œnoâ€ tokens and use binary crossâ€‘entropy or listwise losses for trainingã€967128664425397â€ L118-L160ã€‘ and provides the dataset format for trainingã€967128664425397â€ L182-L228ã€‘.
* Sentence Transformers training guide â€“ emphasises mining hard negatives and using appropriate loss functions for crossâ€‘encoder rerankingã€938936884683624â€ L281-L334ã€‘.
